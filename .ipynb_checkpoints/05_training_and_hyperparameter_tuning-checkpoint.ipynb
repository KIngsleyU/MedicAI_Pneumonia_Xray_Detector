{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d2d7874-1848-4dbf-9d68-57c2511e3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris, get_execution_role\n",
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner, IntegerParameter\n",
    "from sagemaker.session import TrainingInput\n",
    "import boto3\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa60e128-bbfe-4edd-ad1e-e4f2407690e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"medical--ai-chest-xray\"\n",
    "region = \"us-east-1\"\n",
    "role_arn = \"arn:aws:s3:::medical--ai-chest-xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ca65cdf-16d1-4ce2-b6e0-9bd94daf88ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:1'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Creating a SageMaker session object. \n",
    "This is a high-level interface for interacting with SageMaker resources.\n",
    "\"\"\"\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "\"\"\"\n",
    "Retrieving the URI for the SageMaker built-in image classification algorithm container.\n",
    "'region' specifies the AWS region (obtained from the current boto3 session),\n",
    "and 'framework' specifies the built-in algorithm name (\"image-classification\").\n",
    "\"\"\"\n",
    "algorithm_image = image_uris.retrieve(region=boto3.Session().region_name,\n",
    "                                      framework=\"image-classification\")\n",
    "\n",
    "\"\"\"\n",
    "Defining the S3 location where the trained model artifacts will be stored.\n",
    "'bucket' should be replaced with the name of your S3 bucket.\n",
    "\"\"\"\n",
    "s3_output_location = f\"s3://{bucket}/models/image_model\"\n",
    "\n",
    "\"\"\"\n",
    "Output the algorithm image URI\n",
    "\"\"\"\n",
    "algorithm_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e5d64dfc-aa8c-4684-957a-16380735022c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::968765902470:role/service-role/AmazonSageMaker-ExecutionRole-20240509T201394'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrieve the execution role\n",
    "This function call retrieves the IAM role associated with the SageMaker notebook instance,\n",
    "which is necessary for SageMaker to access AWS resources on your behalf.\n",
    "\"\"\"\n",
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e9ad0239-225f-4a21-85e1-5d80083f3c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.estimator.Estimator at 0x7f57392fabf0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_classifier_model = sagemaker.estimator.Estimator(\n",
    "    algorithm_image,          # The URI of the Docker image for the SageMaker \n",
    "                              # built-in image classification algorithm. \n",
    "                              # Required to specify which algorithm to use. \n",
    "\n",
    "    role=role,                # The IAM role that SageMaker uses to access AWS \n",
    "                              # resources. Required for SageMaker to have \n",
    "                              # the necessary permissions.\n",
    "    \n",
    "    instance_count=1,         # The number of EC2 instances to use for training. \n",
    "                              # For simple tasks, 1 instance is usually sufficient.\n",
    "    \n",
    "    instance_type=\"ml.p2.xlarge\", # The type of EC2 instance to use for training. \n",
    "                                  # 'ml.p2.xlarge' provides GPU support, which is \n",
    "                                  # beneficial for compute-intensive tasks like \n",
    "                                  # image classification.\n",
    "    \n",
    "    volume_size=50,           # The size (in GB) of the EBS volume to attach to \n",
    "                              # the instances. Determines the storage space for \n",
    "                              # training data and model artifacts.\n",
    "    \n",
    "    max_run=432000,          # The maximum allowed runtime for the training job \n",
    "                              # (in seconds). Helps prevent runaway jobs that \n",
    "                              # exceed expected training times.\n",
    "    \n",
    "    input_mode=\"File\",        # The input mode for the training data. 'File' mode \n",
    "                              # reads data from an S3 bucket. Suitable for large \n",
    "                              # datasets stored in S3.\n",
    "    \n",
    "    output_path=s3_output_location, # The S3 path where the trained model artifacts \n",
    "                                    # will be stored. Ensures that the output model \n",
    "                                    # is saved and accessible for future use.\n",
    "    \n",
    "    sagemaker_session=sess    # The SageMaker session object that manages \n",
    "                              # interactions with SageMaker. Required to link the \n",
    "                              # Estimator with the current SageMaker session.\n",
    ")\n",
    "img_classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a0c6ba9c-a29f-4866-a3d2-6e480d8cd07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(glob.glob(\"data/chest_xray/train/*.jpeg\"))\n",
    "count=5216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91285507-2cfb-43b7-b350-dd7f32055b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for the image classification model\n",
    "img_classifier_model.set_hyperparameters(\n",
    "    image_shape=\"3,224,224\",            # Specifies the input image dimensions. \n",
    "                                        # '3' refers to the number of color channels (RGB), \n",
    "                                        # '224,224' are the height and width in pixels.\n",
    "    \n",
    "    num_classes=2,                      # The number of classes in the classification problem. \n",
    "                                        # Adjust this according to your specific use case.\n",
    "    \n",
    "    use_pretrained_model=1,             # Use a pre-trained model to start training. \n",
    "                                        # '1' means True, which can speed up training and \n",
    "                                        # improve accuracy.\n",
    "    \n",
    "    num_training_samples=count,         # The total number of training samples. \n",
    "                                        # 'count' should be defined with the actual number of samples.\n",
    "\n",
    "    augmentation_type=\"crop_color_transform\",  # cropping: Randomly cropping \n",
    "                                                    # images can help the model \n",
    "                                                    # learn to focus on different parts of the image\n",
    "                                                    # crop and resize back to required size\n",
    "                                                    # Adjusting the brightness, contrast, \n",
    "                                                    # saturation, and hue of images helps \n",
    "                                                    # the model become robust to changes \n",
    "                                                    # in lighting conditions and \n",
    "                                                    # color variations. How: Apply \n",
    "                                                    # random adjustments to the color \n",
    "                                                    # properties of the image.\n",
    "    \n",
    "    \n",
    "    epochs=15,                          # The number of complete passes through the training dataset. \n",
    "                                        # More epochs can lead to better accuracy, up to a certain point.\n",
    "    \n",
    "    early_stopping=True,                # Enable early stopping to prevent overfitting. \n",
    "                                        # Training stops when performance on a validation set \n",
    "                                        # stops improving.\n",
    "    \n",
    "    early_stopping_min_epochs=8,        # The minimum number of epochs to run before considering \n",
    "                                        # early stopping.\n",
    "    \n",
    "    early_stopping_patience=5,          # The number of epochs with no improvement after which \n",
    "                                        # training will be stopped. A patience of 5 means \n",
    "                                        # training will stop if there is no improvement for 5 epochs.\n",
    "    \n",
    "    early_stopping_tolerance=0.0,       # The threshold for considering an improvement in performance. \n",
    "                                        # A tolerance of 0.0 means any improvement is considered.\n",
    "    \n",
    "    lr_scheduler_factor=0.1,            # Factor by which the learning rate will be reduced. \n",
    "                                        # '0.1' means the learning rate is multiplied by 0.1.\n",
    "    \n",
    "    lr_scheduler_step=\"8,10,12\"         # Epochs at which the learning rate will be reduced. \n",
    "                                        # Specifies a schedule for when to reduce the learning rate.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d6e5416-8bf1-4b8f-9c5f-fdc92179d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.01, 0.1),\n",
    "    \"mini_batch_size\": CategoricalParameter([8, 16, 23]),\n",
    "    \"optimizer\": CategoricalParameter([\"sgd\", \"adam\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f7745a3e-6361-4af3-a0b9-83e3ee10c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HyperparameterTuner object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=img_classifier_model,             # The SageMaker Estimator object to be tuned.\n",
    "    \n",
    "    objective_metric_name=\"validation:accuracy\",# The metric used to evaluate the model's performance.\n",
    "                                                # Here, we aim to maximize the validation accuracy.\n",
    "    \n",
    "    hyperparameter_ranges=hyperparameter_ranges,# The range of hyperparameters to search.\n",
    "                                                # 'hyperparameter_ranges' should be a dictionary \n",
    "                                                # defining the parameters and their ranges.\n",
    "    \n",
    "    objective_type=\"Maximize\",                  # The type of objective metric.\n",
    "                                                # \"Maximize\" means we aim to maximize the validation accuracy.\n",
    "    \n",
    "    max_jobs=5,                                 # The total number of hyperparameter tuning jobs to run.\n",
    "                                                # This limits the number of different hyperparameter combinations to try.\n",
    "    \n",
    "    max_parallel_jobs=1                         # The maximum number of parallel training jobs.\n",
    "                                                # Limits how many training jobs can run at the same time.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a9411b4-82a0-472f-b9b2-52c9d2dd9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input data configuration for the training job\n",
    "model_input = {\n",
    "    \"train\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{bucket}/train\",    # S3 URI for the training data\n",
    "        content_type=\"application/x-image\" # Content type for the validation dat\n",
    "    ),\n",
    "    \"validation\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{bucket}/test\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"train_lst\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{bucket}/train.lst\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"validation_lst\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{bucket}/test.lst\",\n",
    "        content_type=\"application/x-image\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74caa988-382a-4125-93ad-5cd307396907",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix= \"classifier\"\n",
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\",\n",
    "                          time.gmtime())\n",
    "job_name = job_name_prefix+timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c5196-a929-4590-b336-9a1f2ab65654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "tuner.fit(inputs=model_input, \n",
    "         job_name=job_name,\n",
    "         log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37625920-b320-4218-b5e3-dc833fb728f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3://medical--ai-chest-xray/test.lst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
